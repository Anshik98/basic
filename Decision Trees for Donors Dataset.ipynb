{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jKMeoHjWN-3k"
   },
   "source": [
    "# <b> Decison Trees</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjA-ZU-TqVK1"
   },
   "source": [
    "<font color='red'><b> TF-IDFW2V</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvBr2z6iqW9V"
   },
   "source": [
    "<b>Tfidf w2v (w1,w2..) = (tfidf(w1) * w2v(w1) + tfidf(w2) * w2v(w2) + …)  /    (tfidf(w1) + tfidf(w2) + …)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zRAy5UzOvi_a"
   },
   "source": [
    "<b>(Optional) Please check course video on [AVgw2V and TF-IDFW2V ](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2916/avg-word2vec-tf-idf-weighted-word2vec/3/module-3-foundations-of-natural-language-processing-and-machine-learning)for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IB2uk5LwtBlO"
   },
   "source": [
    "<font color='blue'><b>Glove vectors </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j697XLZGtCnz"
   },
   "source": [
    "<b>for glove vectors , please check  [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) and [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) for more details.</b><br>\n",
    "\n",
    "Download glove vectors from this [link ](https://drive.google.com/file/d/1lDca_ge-GYO0iQ6_XDLWePQFMdAA2b8f/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ufHLoACuoHw"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r\"C:\\Users\\anshi\\Downloads\\glove_vectors\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTJ7Et5hxpZS"
   },
   "source": [
    "# <font color='red'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACUkHex3N-3m"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_essay (TFIDF) + Sentiment scores(preprocessed_essay)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (best `depth` in range [1, 5, 10, 50], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>min_sample_split</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n",
    "        <ul>\n",
    "            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n",
    "            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n",
    "            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n",
    "        </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqWyfo1Sx8ua"
   },
   "source": [
    "# <font color='red'><b> Task - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xddr3kChx-ew"
   },
   "source": [
    "For this task consider **set-1** features.\n",
    "\n",
    "*  Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n",
    "   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n",
    "*  You need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n",
    "  **Note**: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n",
    "  </li>\n",
    "    <br>\n",
    "You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZ-qDp6KxNj0"
   },
   "source": [
    "<font color='blue'><b>Hint for calculating Sentiment scores</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "2IHTExN4xd5p",
    "outputId": "fe01a7c9-e5cc-4ebe-d4d8-e0b0278d103c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\anshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YKZIvFYBxaaD",
    "outputId": "d9dcffbf-971d-4220-c03e-bf01cd81bdd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg: 0.01, neu: 0.745, pos: 0.245, compound: 0.9975, "
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "for_sentiment = 'a person is a person no matter how small dr seuss i teach the smallest students with the biggest enthusiasm \\\n",
    "for learning my students learn in many different ways using all of our senses and multiple intelligences i use a wide range\\\n",
    "of techniques to help all my students succeed students in my class come from a variety of different backgrounds which makes\\\n",
    "for wonderful sharing of experiences and cultures including native americans our school is a caring community of successful \\\n",
    "learners which can be seen through collaborative student project based learning in and out of the classroom kindergarteners \\\n",
    "in my class love to work with hands on materials and have many different opportunities to practice a skill before it is\\\n",
    "mastered having the social skills to work cooperatively with friends is a crucial aspect of the kindergarten curriculum\\\n",
    "montana is the perfect place to learn about agriculture and nutrition my students love to role play in our pretend kitchen\\\n",
    "in the early childhood classroom i have had several kids ask me can we try cooking with real food i will take their idea \\\n",
    "and create common core cooking lessons where we learn important math and writing concepts while cooking delicious healthy \\\n",
    "food for snack time my students will have a grounded appreciation for the work that went into making the food and knowledge \\\n",
    "of where the ingredients came from as well as how it is healthy for their bodies this project would expand our learning of \\\n",
    "nutrition and agricultural cooking recipes by having us peel our own apples to make homemade applesauce make our own bread \\\n",
    "and mix up healthy plants from our classroom garden in the spring we will also create our own cookbooks to be printed and \\\n",
    "shared with families students will gain math and literature skills as well as a life long enjoyment for healthy cooking \\\n",
    "nannan'\n",
    "ss = sid.polarity_scores(for_sentiment)\n",
    "\n",
    "for k in ss:\n",
    "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "# we can use these 4 things as features/attributes (neg, neu, pos, compound)\n",
    "# neg: 0.0, neu: 0.753, pos: 0.247, compound: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6FUMj5TN-3y"
   },
   "source": [
    "<h1>1. Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQmid3VAN-31"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqY4ES_3N-33"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(r\"C:\\Users\\anshi\\Downloads\\preprocessed_data-DT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects clean_categories  \\\n",
       "0                                            53     math_science   \n",
       "\n",
       "                  clean_subcategories  \\\n",
       "0  appliedsciences health_lifescience   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['project_is_approved'].values\n",
    "X = data.drop(['project_is_approved'], axis=1)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Spliting\n",
    "Since we will be applying grid search so wont need to separate cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 51) (73196,)\n",
      "(36052, 51) (36052,)\n",
      "['ak', 'al', 'ar', 'az', 'ca', 'co', 'ct', 'dc', 'de', 'fl', 'ga', 'hi', 'ia', 'id', 'il', 'in', 'ks', 'ky', 'la', 'ma', 'md', 'me', 'mi', 'mn', 'mo', 'ms', 'mt', 'nc', 'nd', 'ne', 'nh', 'nj', 'nm', 'nv', 'ny', 'oh', 'ok', 'or', 'pa', 'ri', 'sc', 'sd', 'tn', 'tx', 'ut', 'va', 'vt', 'wa', 'wi', 'wv', 'wy']\n",
      "====================================================================================================\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['school_state'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_state_ohe = vectorizer.transform(X_train['school_state'].values)\n",
    "X_test_state_ohe = vectorizer.transform(X_test['school_state'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_state_ohe.shape, y_train.shape)\n",
    "print(X_test_state_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "for i in vectorizer.get_feature_names():\n",
    "    tfidf_features.append(i)\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 5) (73196,)\n",
      "(36052, 5) (36052,)\n",
      "['dr', 'mr', 'mrs', 'ms', 'teacher']\n",
      "====================================================================================================\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['teacher_prefix'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_teacher_ohe = vectorizer.transform(X_train['teacher_prefix'].values)\n",
    "X_test_teacher_ohe = vectorizer.transform(X_test['teacher_prefix'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_teacher_ohe.shape, y_train.shape)\n",
    "print(X_test_teacher_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "for i in vectorizer.get_feature_names():\n",
    "    tfidf_features.append(i)\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 4) (73196,)\n",
      "(36052, 4) (36052,)\n",
      "['grades_3_5', 'grades_6_8', 'grades_9_12', 'grades_prek_2']\n",
      "====================================================================================================\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train['project_grade_category'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_grade_ohe = vectorizer.transform(X_train['project_grade_category'].values)\n",
    "X_test_grade_ohe = vectorizer.transform(X_test['project_grade_category'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_grade_ohe.shape, y_train.shape)\n",
    "print(X_test_grade_ohe.shape, y_test.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"=\"*100)\n",
    "for i in vectorizer.get_feature_names():\n",
    "    tfidf_features.append(i)\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing of numerical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "====================================================================================================\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['price'].values.reshape(-1,1))\n",
    "\n",
    "X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_test_price_norm = normalizer.transform(X_test['price'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_price_norm.shape, y_train.shape)\n",
    "print(X_test_price_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_features.append(\"price\")\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After vectorizations\n",
      "(73196, 1) (73196,)\n",
      "(36052, 1) (36052,)\n",
      "====================================================================================================\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "\n",
    "X_train_projectcount_norm = normalizer.transform(X_train['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "X_test_projectcount_norm = normalizer.transform(X_test['teacher_number_of_previously_posted_projects'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_projectcount_norm.shape, y_train.shape)\n",
    "print(X_test_projectcount_norm.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "tfidf_features.append(\"teacher_number_of_previously_posted_projects\")\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing of textual features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73196, 8) (73196,)\n",
      "(36052, 8) (36052,)\n",
      "====================================================================================================\n",
      "After vectorizations\n",
      "(73196, 146487) (73196,)\n",
      "(36052, 146487) (36052,)\n",
      "====================================================================================================\n",
      "146549\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,2))\n",
    "vectorizer.fit(X_train['essay'].values)\n",
    "\n",
    "X_train_essay_bow = vectorizer.transform(X_train['essay'].values)\n",
    "X_test_essay_bow = vectorizer.transform(X_test['essay'].values)\n",
    "\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_essay_bow.shape, y_train.shape)\n",
    "print(X_test_essay_bow.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "for i in vectorizer.get_feature_names():\n",
    "    tfidf_features.append(i)\n",
    "print(len(tfidf_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "essays = X_train['essay']\n",
    "essays_sentiments_train=[]\n",
    "for essay in essays:\n",
    "    res = sid.polarity_scores(essay)\n",
    "    essays_sentiments_train.append(res['compound'])\n",
    "    \n",
    "X_train['essay_sentiments'] = essays_sentiments_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "essays = X_test['essay']\n",
    "essays_sentiments_test=[]\n",
    "for essay in essays:\n",
    "    res = sid.polarity_scores(essay)\n",
    "    essays_sentiments_test.append(res['compound'])\n",
    "    \n",
    "X_test['essay_sentiments'] = essays_sentiments_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_test=X_test['essay_sentiments'].values.reshape(-1,1)\n",
    "sentiment_train=X_train['essay_sentiments'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data merging - Set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "x_train_set1 = hstack((X_train_state_ohe,X_train_teacher_ohe,X_train_grade_ohe,X_train_price_norm,\n",
    "                            X_train_projectcount_norm,X_train_essay_bow,sentiment_train)).tocsr()\n",
    "x_test_set1=hstack((X_test_state_ohe,X_test_teacher_ohe,X_test_grade_ohe,X_test_price_norm,\n",
    "                            X_test_projectcount_norm,X_test_essay_bow,sentiment_test)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_tfidf = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "parameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\n",
    "classifier = GridSearchCV(dt_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n",
    "classifier.fit(x_train_set1,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is best score in decision trees?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc= classifier.cv_results_['mean_train_score']\n",
    "train_auc_std= classifier.cv_results_['std_train_score']\n",
    "\n",
    "cv_auc = classifier.cv_results_['mean_test_score']\n",
    "cv_auc_std= classifier.cv_results_['std_test_score']\n",
    "\n",
    "bestMaxDepth_set1=classifier.best_params_['max_depth']\n",
    "bestMinSampleSplit_set1=classifier.best_params_['min_samples_split']\n",
    "\n",
    "print(\"Depth: \",classifier.best_params_['max_depth'])\n",
    "print(\"Best Score: \",classifier.best_score_)\n",
    "print(\"Sample Split: \",classifier.best_params_['min_samples_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/56302647/how-to-plot-a-heatmap-and-find-best-hyperparameter-for-decision-tree-after-grids\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "results = pd.DataFrame.from_dict(classifier.cv_results_)\n",
    "\n",
    "max_scores = results.groupby(['param_min_samples_split', 'param_max_depth']).max()\n",
    "max_scores = max_scores.unstack()[['mean_test_score', 'mean_train_score']]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "sns.heatmap(max_scores.mean_train_score, annot=True, fmt='.4g',ax=ax[0]);\n",
    "sns.heatmap(max_scores.mean_test_score, annot=True, fmt='.4g',ax=ax[1]);\n",
    "\n",
    "ax[0].set_title('Train Set')\n",
    "ax[1].set_title('CV Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "classifier_ = DecisionTreeClassifier(random_state=42,class_weight='balanced',min_samples_split=500,max_depth=10)\n",
    "classifier_.fit(x_train_set1,y_train)\n",
    "\n",
    "y_pred=classifier_.predict(x_test_set1)\n",
    "y_train_pred = classifier_.predict_proba(x_train_set1)[:,1]     \n",
    "y_test_pred = classifier_.predict_proba(x_test_set1)[:,1] \n",
    "\n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC ROC Curve \")\n",
    "plt.grid(color='black', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def find_best_threshold(threshould, fpr, tpr):\n",
    "    t = threshould[np.argmax(tpr*(1-fpr))]\n",
    "    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n",
    "    print(\"the maximum value of tpr*(1-fpr)\", np.round(max(tpr*(1-fpr)),3), \"for threshold\", np.round(t,3))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_best_t(proba, threshould):\n",
    "    predictions = []\n",
    "    for i in proba:\n",
    "        if i>=threshould:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n",
    "print(\"Threshold\", best_t)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)).ravel())\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_test = pd.DataFrame(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(cf_test,annot=True, annot_kws={\"size\": 20},fmt =\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # for masking i.e print word in the pattern we want\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'Youtube04-Eminem.csv' files\n",
    "# using encoding = \"latin-1\" to get vertical words arrangement along with horizontal once\n",
    "# dataFrame = pd.read_csv(r\"Youtube04-Eminem.csv\", encoding = \"latin-1\")\n",
    "# dataFrame.head()\n",
    "def printWordCloud(FP_list):\n",
    "    comment_words = ''\n",
    "    stopwords = set(STOPWORDS) \n",
    "\n",
    "    # for val in dataFrame.CONTENT: \n",
    "    for val in FP_list: \n",
    "\n",
    "        # typecaste each val to string \n",
    "        val = str(val) \n",
    "\n",
    "        # split the value \n",
    "        tokens = val.split() \n",
    "\n",
    "        # Converts each token into lowercase \n",
    "        for i in range(len(tokens)): \n",
    "            tokens[i] = tokens[i].lower() \n",
    "\n",
    "        for words in tokens: \n",
    "            comment_words = comment_words + words + ' '\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(width = 500, height = 500, \n",
    "                    background_color ='white', \n",
    "                    stopwords = stopwords, \n",
    "                    min_font_size = 10).generate(comment_words) \n",
    "\n",
    "    # plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Positive Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_datapoints=[]\n",
    "for i in range(len(y_test)):    \n",
    "    if(y_test[i]==0 and y_pred[i]==1 ):        \n",
    "        FP_datapoints.append(i)\n",
    "FP_datapoints_essay=[]\n",
    "FP_datapoints_price=[]\n",
    "FP_datapoints_previous=[]\n",
    "\n",
    "for i in FP_datapoints:\n",
    "    FP_datapoints_essay.append(X_test['essay'].values[i])\n",
    "    FP_datapoints_price.append(X_test['price'].values[i])\n",
    "    FP_datapoints_previous.append(X_test['teacher_number_of_previously_posted_projects'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printWordCloud(FP_datapoints_essay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(FP_datapoints_price)\n",
    "plt.title('Box Plot for PRICE in False Positives')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(FP_datapoints_previous)\n",
    "plt.title('PDF for Teacher number who previously posted projects in False Positives')\n",
    "plt.xlabel('Teacher number who previously posted projects')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def TFIDF_W2V(feature, unique_words,idf_dict):\n",
    "    doc=[]\n",
    "    for sentence in feature:\n",
    "        vector=np.zeros(300)\n",
    "        tf_idf_weight=0\n",
    "        \n",
    "        for word in sentence.split():\n",
    "            if(word in glove_words) and (word in unique_words):\n",
    "                w2vec = model[word] \n",
    "                tf=sentence.count(word)/len(sentence.split())\n",
    "                idf=idf_dict[word]\n",
    "                tf_idf =tf*idf\n",
    "                vector += (w2vec * tf_idf)\n",
    "                tf_idf_weight += tf_idf\n",
    "        if tf_idf_weight != 0:\n",
    "            vector /= tf_idf_weight\n",
    "            \n",
    "        doc.append(vector)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(X_train['essay'])\n",
    "\n",
    "essay_dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "essay_unique_words = set(tfidf_model.get_feature_names())\n",
    "\n",
    "train_essay=TFIDF_W2V(X_train['essay'],essay_unique_words,essay_dictionary)\n",
    "test_essay=TFIDF_W2V(X_test['essay'],essay_unique_words,essay_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_essay.shape)\n",
    "print(test_essay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_set2 = hstack((X_train_state_ohe,X_train_teacher_ohe,X_train_grade_ohe,X_train_price_norm,\n",
    "                            X_train_projectcount_norm,train_essay,sentiment_train)).tocsr()\n",
    "x_test_set2=hstack((X_test_state_ohe,X_test_teacher_ohe,X_test_grade_ohe,X_test_price_norm,\n",
    "                            X_test_projectcount_norm,test_essay,sentiment_test)).tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tfidf_set2 = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "parameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\n",
    "classifier_set2 = GridSearchCV(dt_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n",
    "classifier_set2.fit(x_train_set2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc= classifier_set2.cv_results_['mean_train_score']\n",
    "train_auc_std= classifier_set2.cv_results_['std_train_score']\n",
    "\n",
    "cv_auc = classifier_set2.cv_results_['mean_test_score']\n",
    "cv_auc_std= classifier_set2.cv_results_['std_test_score']\n",
    "\n",
    "bestMaxDepth_set2=classifier_set2.best_params_['max_depth']\n",
    "bestMinSampleSplit_set2=classifier_set2.best_params_['min_samples_split']\n",
    "\n",
    "print(\"Depth: \",classifier_set2.best_params_['max_depth'])\n",
    "print(\"Best Score: \",classifier_set2.best_score_)\n",
    "print(\"Sample Split: \",classifier_set2.best_params_['min_samples_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_set2 = pd.DataFrame.from_dict(classifier_set2.cv_results_)\n",
    "\n",
    "max_scores_set2 = results_set2.groupby(['param_min_samples_split', 'param_max_depth']).max()\n",
    "max_scores_set2 = max_scores_set2.unstack()[['mean_test_score', 'mean_train_score']]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "sns.heatmap(max_scores_set2.mean_train_score, annot=True, fmt='.4g',ax=ax[0]);\n",
    "sns.heatmap(max_scores_set2.mean_test_score, annot=True, fmt='.4g',ax=ax[1]);\n",
    "\n",
    "ax[0].set_title('Train Set-2')\n",
    "ax[1].set_title('CV Set-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with best hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "classifier_2 = DecisionTreeClassifier(random_state=42,class_weight='balanced',min_samples_split=500,max_depth=5)\n",
    "classifier_2.fit(x_train_set2,y_train)\n",
    "\n",
    "y_pred_set2=classifier_2.predict(x_test_set2)\n",
    "y_train_pred_set2 = classifier_2.predict_proba(x_train_set2)[:,1]     \n",
    "y_test_pred_set2 = classifier_2.predict_proba(x_test_set2)[:,1] \n",
    "\n",
    "train_fpr2, train_tpr2, tr_thresholds2 = roc_curve(y_train, y_train_pred_set2)\n",
    "test_fpr2, test_tpr2, te_thresholds2 = roc_curve(y_test, y_test_pred_set2)\n",
    "\n",
    "plt.plot(train_fpr2, train_tpr2, label=\"Train AUC =\"+str(auc(train_fpr2, train_tpr2)))\n",
    "plt.plot(test_fpr2, test_tpr2, label=\"Test AUC =\"+str(auc(test_fpr2, test_tpr2)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC ROC Curve \")\n",
    "plt.grid(color='black', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_t = find_best_threshold(tr_thresholds2, train_fpr2, train_tpr2)\n",
    "print(\"Threshold\", best_t)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_pred_set2, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_train, predict_with_best_t(y_train_pred_set2, best_t)).ravel())\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_pred_set2, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_test, predict_with_best_t(y_test_pred_set2, best_t)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_test = pd.DataFrame(confusion_matrix(y_test, predict_with_best_t(y_test_pred_set2, best_t)))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(cf_test,annot=True, annot_kws={\"size\": 20},fmt =\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_datapoints2=[]\n",
    "for i in range(len(y_test)):    \n",
    "    if(y_test[i]==0 and y_pred_set2[i]==1 ):        \n",
    "        FP_datapoints2.append(i)\n",
    "FP_datapoints_essay2=[]\n",
    "FP_datapoints_price2=[]\n",
    "FP_datapoints_previous2=[]\n",
    "\n",
    "for i in FP_datapoints2:\n",
    "    FP_datapoints_essay2.append(X_test['essay'].values[i])\n",
    "    FP_datapoints_price2.append(X_test['price'].values[i])\n",
    "    FP_datapoints_previous2.append(X_test['teacher_number_of_previously_posted_projects'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printWordCloud(FP_datapoints_essay2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(FP_datapoints_price2)\n",
    "plt.title('Box Plot for PRICE in False Positives')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(FP_datapoints_previous2)\n",
    "plt.title('PDF for Teacher number who previously posted projects in False Positives')\n",
    "plt.xlabel('Teacher number who previously posted projects')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero=0\n",
    "for i in range (len(classifier_.feature_importances_)):\n",
    "    if(classifier_.feature_importances_[i]>0):\n",
    "        nonzero=nonzero+1\n",
    "\n",
    "print(nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_sort_train=x_train_set1[:,classifier_.feature_importances_.argsort()[::-1][:nonzero]]\n",
    "nonzero_sort_test=x_test_set1[:,classifier_.feature_importances_.argsort()[::-1][:nonzero]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_sort_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_sort_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tfidf_set3 = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "parameters = {'max_depth':[1, 5, 10, 50],'min_samples_split':[5, 10, 100, 500]}\n",
    "classifier_set3 = GridSearchCV(dt_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n",
    "classifier_set3.fit(nonzero_sort_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc= classifier_set3.cv_results_['mean_train_score']\n",
    "train_auc_std= classifier_set3.cv_results_['std_train_score']\n",
    "\n",
    "cv_auc = classifier_set3.cv_results_['mean_test_score']\n",
    "cv_auc_std= classifier_set3.cv_results_['std_test_score']\n",
    "\n",
    "bestMaxDepth_set2=classifier_set3.best_params_['max_depth']\n",
    "bestMinSampleSplit_set2=classifier_set3.best_params_['min_samples_split']\n",
    "\n",
    "print(\"Depth: \",classifier_set3.best_params_['max_depth'])\n",
    "print(\"Best Score: \",classifier_set3.best_score_)\n",
    "print(\"Sample Split: \",classifier_set3.best_params_['min_samples_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_set3 = pd.DataFrame.from_dict(classifier_set3.cv_results_)\n",
    "\n",
    "max_scores_set3 = results_set3.groupby(['param_min_samples_split', 'param_max_depth']).max()\n",
    "max_scores_set3 = max_scores_set3.unstack()[['mean_test_score', 'mean_train_score']]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "sns.heatmap(max_scores_set3.mean_train_score, annot=True, fmt='.4g',ax=ax[0]);\n",
    "sns.heatmap(max_scores_set3.mean_test_score, annot=True, fmt='.4g',ax=ax[1]);\n",
    "\n",
    "ax[0].set_title('Train Set-3')\n",
    "ax[1].set_title('CV Set-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "classifier_3 = DecisionTreeClassifier(random_state=42,class_weight='balanced',min_samples_split=500,max_depth=10)\n",
    "classifier_3.fit(nonzero_sort_train,y_train)\n",
    "\n",
    "y_pred_set3=classifier_3.predict(nonzero_sort_test)\n",
    "y_train_pred_set3 = classifier_3.predict_proba(nonzero_sort_train)[:,1]     \n",
    "y_test_pred_set3 = classifier_3.predict_proba(nonzero_sort_test)[:,1] \n",
    "\n",
    "train_fpr3, train_tpr3, tr_thresholds3 = roc_curve(y_train, y_train_pred_set3)\n",
    "test_fpr3, test_tpr3, te_thresholds3 = roc_curve(y_test, y_test_pred_set3)\n",
    "\n",
    "plt.plot(train_fpr3, train_tpr3, label=\"Train AUC =\"+str(auc(train_fpr3, train_tpr3)))\n",
    "plt.plot(test_fpr3, test_tpr3, label=\"Test AUC =\"+str(auc(test_fpr3, test_tpr3)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC ROC Curve \")\n",
    "plt.grid(color='black', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_t = find_best_threshold(tr_thresholds3, train_fpr3, train_tpr3)\n",
    "print(\"Threshold\", best_t)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_pred_set3, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_train, predict_with_best_t(y_train_pred_set3, best_t)).ravel())\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_pred_set3, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_test, predict_with_best_t(y_test_pred_set3, best_t)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_test = pd.DataFrame(confusion_matrix(y_test, predict_with_best_t(y_test_pred_set3, best_t)))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(cf_test,annot=True, annot_kws={\"size\": 20},fmt =\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_datapoints3=[]\n",
    "for i in range(len(y_test)):    \n",
    "    if(y_test[i]==0 and y_pred_set2[i]==1 ):        \n",
    "        FP_datapoints3.append(i)\n",
    "FP_datapoints_essay3=[]\n",
    "FP_datapoints_price3=[]\n",
    "FP_datapoints_previous3=[]\n",
    "\n",
    "for i in FP_datapoints3:\n",
    "    FP_datapoints_essay3.append(X_test['essay'].values[i])\n",
    "    FP_datapoints_price3.append(X_test['price'].values[i])\n",
    "    FP_datapoints_previous3.append(X_test['teacher_number_of_previously_posted_projects'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printWordCloud(FP_datapoints_essay3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(FP_datapoints_price3)\n",
    "plt.title('Box Plot for PRICE in False Positives')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(FP_datapoints_previous3)\n",
    "plt.title('PDF for Teacher number who previously posted projects in False Positives')\n",
    "plt.xlabel('Teacher number who previously posted projects')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on Set1 Important features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf=LogisticRegression()\n",
    "grid_values={'penalty':['l1','l2'],'C':[0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "model2=GridSearchCV(estimator=clf,param_grid=grid_values,scoring='roc_auc', verbose=1, cv=3, n_jobs=-1)\n",
    "model2.fit(nonzero_sort_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf=LogisticRegression()\n",
    "grid_values={'penalty':['l2'],'C':[1000]}\n",
    "model2=GridSearchCV(estimator=clf,param_grid=grid_values,scoring='roc_auc', verbose=1, cv=3, n_jobs=-1)\n",
    "model2.fit(nonzero_sort_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=model2.predict(nonzero_sort_test)\n",
    "y_train_prob=model2.predict_proba(nonzero_sort_train)[:,1]\n",
    "y_test_prob = model2.predict_proba(nonzero_sort_test)[:,1] \n",
    "\n",
    "train_fpr4, train_tpr4, tr_thresholds4 = roc_curve(y_train, y_train_prob)\n",
    "test_fpr4, test_tpr4, te_thresholds4 = roc_curve(y_test, y_test_prob)\n",
    "\n",
    "plt.plot(train_fpr4, train_tpr4, label=\"Train AUC =\"+str(auc(train_fpr4, train_tpr4)))\n",
    "plt.plot(test_fpr4, test_tpr4, label=\"Test AUC =\"+str(auc(test_fpr4, test_tpr4)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC ROC Curve \")\n",
    "plt.grid(color='black', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_t = find_best_threshold(tr_thresholds4, train_fpr4, train_tpr4)\n",
    "print(\"Threshold\", best_t)\n",
    "print(\"Train confusion matrix\")\n",
    "print(confusion_matrix(y_train, predict_with_best_t(y_train_prob, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_train, predict_with_best_t(y_train_prob, best_t)).ravel())\n",
    "print(\"Test confusion matrix\")\n",
    "print(confusion_matrix(y_test, predict_with_best_t(y_test_prob, best_t)))\n",
    "print(\"tn, fp, fn, tp\", \"=\", confusion_matrix(y_test, predict_with_best_t(y_test_prob, best_t)).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_test = pd.DataFrame(confusion_matrix(y_test, predict_with_best_t(y_test_prob, best_t)))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(cf_test,annot=True, annot_kws={\"size\": 20},fmt =\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_datapoints4=[]\n",
    "for i in range(len(y_test)):    \n",
    "    if(y_test[i]==0 and y[i]==1 ):        \n",
    "        FP_datapoints4.append(i)\n",
    "FP_datapoints_essay4=[]\n",
    "FP_datapoints_price4=[]\n",
    "FP_datapoints_previous4=[]\n",
    "\n",
    "for i in FP_datapoints4:\n",
    "    FP_datapoints_essay4.append(X_test['essay'].values[i])\n",
    "    FP_datapoints_price4.append(X_test['price'].values[i])\n",
    "    FP_datapoints_previous4.append(X_test['teacher_number_of_previously_posted_projects'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printWordCloud(FP_datapoints_essay4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(FP_datapoints_price4)\n",
    "plt.title('Box Plot for PRICE in False Positives')\n",
    "plt.ylabel('Price')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "sns.distplot(FP_datapoints_previous4)\n",
    "plt.title('PDF for Teacher number who previously posted projects in False Positives')\n",
    "plt.xlabel('Teacher number who previously posted projects')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Vectorizer\", \"Model\", \"Minimum Sample Split\",\"Maximum Dept\", \"Train AUC\", \"Test AUC\"]\n",
    "x.add_row(['tfidf', 'decision tree','500','10','0.68','0.62'])\n",
    "x.add_row(['tfidf-word2vec', 'decision tree','500','5','0.64','0.60'])\n",
    "x.add_row(['non-zero features of set1', 'decision tree','500','10','0.68','0.62'])\n",
    "x.add_row(['non-zero features of set1', 'logistic regression','L2 Penalty','C=1000','0.68','0.62'])\n",
    "\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_DT_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
